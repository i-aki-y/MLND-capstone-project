{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NegativeBinomial (Benchmark)\n",
    "\n",
    "http://blog.drivendata.org/2016/12/23/dengue-benchmark/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from pprint import pprint as pp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from lib import utils\n",
    "from lib import cols\n",
    "from lib import glm_utils\n",
    "\n",
    "from lib import preprocess\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lib/utils.py:184: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df_train_iq = df_dev_iq[is_train_iq]\n",
      "lib/utils.py:185: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df_valid_iq = df_dev_iq[is_valid_iq]\n",
      "lib/utils.py:186: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df_devtest_iq = df_dev_iq[is_devtest_iq]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sj train: 675 lines\t valid: 156 lines\t devtest: 105 lines\t test: 260 lines\n",
      "iq train: 363 lines\t valid: 104 lines\t devtest: 53 lines\t test: 156 lines\n",
      "sj valid: 0.19%\n",
      "iq valid: 0.22%\n"
     ]
    }
   ],
   "source": [
    "ds_sj, ds_iq = preprocess.preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.tools import eval_measures\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "\n",
    "base_formula = \"total_cases ~ 1 + \" \\\n",
    "                    \"reanalysis_specific_humidity_g_per_kg + \" \\\n",
    "                    \"reanalysis_dew_point_temp_k + \" \\\n",
    "                    \"station_min_temp_c + \" \\\n",
    "                    \"station_avg_temp_c\"\n",
    "\n",
    "base_cols = [\"reanalysis_specific_humidity_g_per_kg\", \"reanalysis_dew_point_temp_k\", \"station_min_temp_c\", \"station_avg_temp_c\"]\n",
    "\n",
    "def get_best_model(train, valid, test, model_formula):\n",
    "    # Step 1: specify the form of the model\n",
    "    \n",
    "    grid = 10 ** np.arange(-8, -3, dtype=np.float64)\n",
    "                    \n",
    "    best_alpha = []\n",
    "    best_valid_score = 1000\n",
    "        \n",
    "    # Step 2: Find the best hyper parameter, alpha\n",
    "    for alpha in grid:\n",
    "        model = smf.glm(formula=model_formula,\n",
    "                        data=train,\n",
    "                        family=sm.families.NegativeBinomial(alpha=alpha))\n",
    "\n",
    "        results = model.fit()\n",
    "        predictions = results.predict(valid).astype(int)\n",
    "        valid_score = eval_measures.meanabs(predictions, valid.total_cases)\n",
    "        \n",
    "        test_predictions = results.predict(test).astype(int)\n",
    "        test_score = eval_measures.meanabs(test_predictions, test.total_cases)\n",
    "\n",
    "        if valid_score < best_valid_score:\n",
    "            best_alpha = alpha\n",
    "            best_valid_score = valid_score\n",
    "            best_test_score = test_score\n",
    "\n",
    "    print('best alpha = ', best_alpha)\n",
    "    print('best valid score = ', best_valid_score)\n",
    "    print('best test score = ', best_test_score)\n",
    "            \n",
    "    # Step 3: refit on entire dataset\n",
    "    full_dataset = pd.concat([train, valid, test])\n",
    "    model = smf.glm(formula=model_formula,\n",
    "                    data=full_dataset,\n",
    "                    family=sm.families.NegativeBinomial(alpha=best_alpha))\n",
    "\n",
    "    fitted_model = model.fit()\n",
    "    return fitted_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best alpha =  1e-08\n",
      "best valid score =  25.108974359\n",
      "best test score =  26.6476190476\n",
      "best alpha =  1e-08\n",
      "best valid score =  8.74038461538\n",
      "best test score =  3.90566037736\n"
     ]
    }
   ],
   "source": [
    "model_formula = base_formula\n",
    "col_feats = base_cols + cols.target\n",
    "sj_best_model = get_best_model(ds_sj.df_train[col_feats], ds_sj.df_valid[col_feats], ds_sj.df_devtest[col_feats], model_formula)\n",
    "iq_best_model = get_best_model(ds_iq.df_train[col_feats], ds_iq.df_valid[col_feats], ds_iq.df_devtest[col_feats], model_formula)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glm_utils.save_result(sj_best_model, iq_best_model, col_feats, col_feats, ds_sj.df_test, ds_iq.df_test, \"NegativeBinomial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best alpha =  1e-08\n",
      "best valid score =  21.9615384615\n",
      "best test score =  24.4952380952\n",
      "best alpha =  1e-08\n",
      "best valid score =  9.13461538462\n",
      "best test score =  3.81132075472\n"
     ]
    }
   ],
   "source": [
    "model_formula = \"total_cases ~ 1 + pca_1 + pca_2 + pca_3 + pca_4 + pca_5 + pca_6 + pca_7\"\n",
    "col_pca_feats = cols.get_pca_feats(7)\n",
    "col_feats = col_pca_feats + cols.target\n",
    "sj_best_model = get_best_model(ds_sj.df_train[col_feats], ds_sj.df_valid[col_feats], ds_sj.df_devtest[col_feats], model_formula)\n",
    "iq_best_model = get_best_model(ds_iq.df_train[col_feats], ds_iq.df_valid[col_feats], ds_iq.df_devtest[col_feats], model_formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glm_utils.save_result(sj_best_model, iq_best_model, col_feats, col_feats, ds_sj.df_test, ds_iq.df_test, \"NegativeBinomial_PCA\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
